---
title: "LAB 1"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The data file optdigits.csv contains information about normalized bitmaps of
handwritten digits from a preprinted form from a total of 43 people. The data were
first derived as 32x32 bitmaps which were then divided into nonoverlapping blocks
of 4x4 and the number of on pixels are counted in each block. This has generated the
resulting image of size 8x8 where each element is an integer in the range 0..16.
Accordingly, each row in the data file is a sequence corresponding to 8x8 matrix,
and the last element shows the actual digit from 0 to 9

1. Import the data into R and divide it into training, validation and test sets
(50%/25%/25%)
```{r, echo=FALSE, warning=FALSE, results='hide'}

# Read in the optdigits.csv file and store the data in a variable named "data"
# Set the "header" argument to "F" to specify that the file does not contain a header row
data = read.csv("optdigits.csv", header = F)

# Get the number of rows in the data and store it in a variable named "n"
n = dim(data)[1]

# Set the seed of the random number generator to 12345
set.seed(12345)

# Randomly select 50% of the rows in the data and store them in a variable named "train"
id=sample(1:n, floor(n*0.5))
train=data[id,]

# Get the remaining rows that were not selected for the training set
id1=setdiff(1:n, id)
set.seed(12345)

# Randomly select 25% of the remaining rows and store them in a variable named "valid"
id2=sample(id1, floor(n*0.25))
valid=data[id2,]

# Get the remaining rows that were not selected for the training or validation sets
id3=setdiff(id1,id2)

# Store the remaining rows in a variable named "test"
test=data[id3,]

# Fit a k-NN classifier to the training data, with k=30 and a rectangular kernel
# Use the classifier to predict the class labels for the training data and store the predictions in "k_train"
k_train <- kknn(as.factor(V65)~., train = train, test = train, k = 30, kernel = "rectangular")

# Use the classifier to predict the class labels for the validation data and store the predictions in "k_valid"
k_valid <- kknn(as.factor(V65)~., train = train, test = valid, k = 30, kernel = "rectangular")

# Use the classifier to predict the class labels for the test data and store the predictions in "k_test"
k_test <- kknn(as.factor(V65)~., train = train, test = test, k = 30, kernel = "rectangular")

# Create a confusion matrix for the training data by comparing the predicted labels to the true labels
train_table = table(k_train$fitted.values, train$V65)

# Create a confusion matrix for the validation data by comparing the predicted labels to the true labels
valid_table = table(k_valid$fitted.values, valid$V65)

# Create a confusion matrix for the test data by comparing the predicted labels to the true labels
test_table = table(k_test$fitted.values, as.factor(test$V65))

# Define a function to calculate the misclassification rate for a given set of predictions and true labels
missclass = function(X, X1) {
  n = length(X)
  return (1 - sum(diag(table(X1, X)))/n)
}

missclass_test = missclass(k_test$fitted.values, as.factor(test$V65))
missclass_train = missclass(train$V65, k_train$fitted.values)
missclass_valid = missclass(k_valid$fitted.values, valid$V65)
```

Confusion matrices

```{r, echo=T}
test_table
train_table 
```

Missclass Values
```{r, echo=T}
missclass_test
missclass_train


```


Comment on the quality of predictions for different digits and on the overall prediction quality.

Some numbers have worse predictions than others, for example number 4 was predicted to be a 7 a couple of times. 
This is most likely due to slopy writing that makes different numbers look more similar to others. 
The overall prediction quality is good.

#Exercise 3

3. Find any 2 cases of digit “8” in the training data which were easiest to classify
and 3 cases that were hardest to classify (i.e. having highest and lowest
probabilities of the correct class). Reshape features for each of these cases as
matrix 8x8 and visualize the corresponding digits (by using e.g. heatmap()
function with parameters Colv=NA and Rowv=NA) and comment on whether
these cases seem to be hard or easy to recognize visually

```{r, echo=F, fig.show='hide'}

# Get the probability of each training sample being classified as the digit "8"
prob_eight <- k_train$prob[, 9]

# Order the probabilities in decreasing order and store the resulting indices in "ordered_high"
# Keep only the top two indices
ordered_high <- order(prob_eight, decreasing = T)
ordered_high <- ordered_high[1:2]

# Order the probabilities in increasing order and store the resulting indices in "ordered_low"
ordered_low <- order(prob_eight, decreasing = F)

# Get the indices of the "ordered_low" vector that correspond to samples with a non-zero probability of being classified as "8"
ordered_index <- which(prob_eight > 0)

# Create an empty vector to store the indices of the first three samples in "ordered_low" that are actually the digit "8"
ordered_low_eights <- c()

# Set an index variable to 1
index = 1

# Use a loop to append the first three indices from "ordered_low" that correspond to samples that are the digit "8" to "ordered_low_eights"
while(length(ordered_low_eights) < 3) {
  # Get the class label of the current sample
  value <-train[ordered_low[index], 65]
  
  # If the sample is the digit "8", append its index to "ordered_low_eights"
  if (value == 8) {
    ordered_low_eights <- append(ordered_low_eights, ordered_low[index])
  }
  
  # Increment the index variable
  index <- index + 1
}

# Print the "ordered_low_eights" vector
print(ordered_low_eights)

# Combine the "ordered_low_eights" and "ordered_high" vectors to create the "ordered_matrix" vector
ordered_matrix <- c(ordered_low_eights, ordered_high)

# Use a loop to create a heatmap for each of the samples corresponding to the indices in the "ordered_matrix" vector
for (i in ordered_matrix){
  # Reshape the sample's pixel values into a 8x8 matrix
  my_heatmap <- matrix(as.numeric(train[i,1:64]), nrow=8,ncol=8, byrow = T)
  
  # Create a heatmap of the matrix and set the main title to the true class label of the sample
  heatmap(my_heatmap, Colv = NA, Rowv = NA, main =train[i, 65])
}

```

The eights that were easy to classify are also very easy to see as eights. 
The eights that were most difficult to spot are very difficult to see without knowing that they're eights beforehand. 


#Exercise 4

Fit a K-nearest neighbor classifiers to the training data for different values of 𝐾𝐾 =
1,2, … , 30 and plot the dependence of the training and validation misclassification
errors on the value of K (in the same plot). How does the model complexity
change when K increases and how does it affect the training and validation 
errors? Report the optimal 𝐾𝐾 according to this plot. Finally, estimate the test error
for the model having the optimal K, compare it with the training and validation
errors and make necessary conclusions about the model quality.
```{r, echo=F, fig.show='hide'}
# Initialize two empty vectors to store the misclassification rates for the training and validation sets
# for different values of k
train_miss_error <- numeric(30)
val_miss_error <- numeric(30)

# Use a loop to fit a k-NN classifier to the training data for each value of k from 1 to 30
# Use the classifier to predict the class labels for the training and validation sets
# Calculate the misclassification rate for each set and store the results in the "train_miss_error" and "val_miss_error" vectors
for (i in 1:30){
  k_valid <- kknn(as.factor(V65)~., train = train, test = valid, k = i, kernel = "rectangular")
  k_train <- kknn(as.factor(V65)~., train = train, test = train, k = i, kernel = "rectangular")
  
  train_miss_error[i] <- missclass(k_train$fitted.values, train$V65)
  val_miss_error[i] <- missclass(k_valid$fitted.values, valid$V65)
}

# Create a plot of the "train_miss_error" and "val_miss_error" vectors
# Use k as the x-axis and the misclassification rate as the y-axis
plot = plot(c(1:30), train_miss_error, ylab = "train_miss_error", xlab = "k", col="pink")
points(c(1:30), val_miss_error, col="green")

# Fit a k-NN classifier to the training data with k=4
# Use the classifier to predict the class labels for the training, validation, and test sets
k_test <- kknn(as.factor(V65)~., train = train, test = test, k = 4, kernel = "rectangular")
k_train <- kknn(as.factor(V65)~., train = train, test = train, k = 4, kernel = "rectangular")
k_valid <- kknn(as.factor(V65)~., train = train, test = valid, k = 4, kernel = "rectangular")

missclass_test = missclass(k_test$fitted.values, as.factor(test$V65))
missclass_train = missclass(train$V65, k_train$fitted.values)
missclass_valid = missclass(k_valid$fitted.values, valid$V65)
```


How does the model complexity change when K increases and how does it affect the training and validation errors?

The model complexity remains the same as k increases, since it's not learning more parameters or features as k increases.
One can argue that the overall complexity is increased since there are more data points added, but this does not mean that the complexity of the model
is higher. 
The best K values are the ones with the lowest missclass error, in this case k=3 and k=4 are the best values. 

We can see that train gives the lowest missclass error because it is the date we made the model from.
Both test and validation is slightly higher which is to be expected.
With the right k value computed with the validation data we get a test error which is just above 2.5% 
Which results in a pretty high quality model.


#Exercise 5

Fit K-nearest neighbor classifiers to the training data for different values of 𝐾𝐾 =
1,2, … , 30, compute the error for the validation data as cross-entropy ( when
computing log of probabilities add a small constant within log, e.g. 1e-15, to
avoid numerical problems) and plot the dependence of the validation error on the
value of 𝐾𝐾. What is the optimal 𝐾𝐾 value here? Assuming that response has
multinomial distribution, why might the cross-entropy be a more suitable choice
of the error function than the misclassification error for this problem?

```{r, echo=F, fig.show='hide'}
cross.entropy <- function(p, phat){
  x <- 0
  for (i in 1:length(p)){
    x <- x + (p[i] * log(phat[i]))
  }
  return(-x)
}

train_miss_error <- as.vector(matrix(0,ncol = 30))
val_miss_error <- as.vector(matrix(0,ncol = 30))

entropy_error <- as.vector(matrix(0,ncol = 30))

for (i in 1:30){
  k_valid <- kknn(as.factor(V65)~., train = train, test = valid, k = i, kernel = "rectangular")
  k_train <- kknn(as.factor(V65)~., train = train, test = train, k = i, kernel = "rectangular")
  
  
  for (j in 0:9){
    cross_val <- valid$V65 == j
    cross_train <- train$V65 == j
    
    bool_val <- (which(cross_val, useNames = T))
    prob_val <- k_valid$prob[cross_val, as.character(j)] + 1e-15
    
    bool_train <- (which(cross_train, useNames = T))
    prob_train <- k_train$prob[cross_train, as.character(j)] + 1e-15
    
    val_miss_error[i] <- val_miss_error[i] + sum(-(bool_val*log(prob_val)))
    train_miss_error[i] <- train_miss_error[i] + sum(-(bool_train*log(prob_train)))
    
    #val_miss_error[i] <- cross.entropy(bool_val, prob_val)
    #train_miss_error[i] <- cross.entropy(bool_train, prob_train)
    
    
    
    entropy_error[i] <- abs(val_miss_error[i]-train_miss_error[i])
  }
  
}

plot(c(1:30), entropy_error, ylab = "cross_entropy_error", xlab = "k", col="blue")
which.min(entropy_error)

```


What is the optimal 𝐾𝐾 value here? Assuming that response has multinomial distribution, why might the cross-entropy be a more suitable choice of the error function than the missclassification error for this problem?

This model might be more suitable because of lower complexity levels than missclass error.
The optimal K value here is K = 5. 

#Assigment 2

The data file parkinson.csv is composed of a range of biomedical voice
measurements from 42 people with early-stage Parkinson's disease recruited to a
six-month trial of a telemonitoring device for remote symptom progression
monitoring. The purpose is to predict Parkinson's disease symptom score (motor
UPDRS) from the following voice characteristics:
• Jitter(%),Jitter(Abs),Jitter:RAP,Jitter:PPQ5,Jitter:DDP - Several measures of
variation in fundamental frequency
• Shimmer,Shimmer(dB),Shimmer:APQ3,Shimmer:APQ5,Shimmer:APQ11,Shi
mmer:DDA - Several measures of variation in amplitude
• NHR,HNR - Two measures of ratio of noise to tonal components in the voice
• RPDE - A nonlinear dynamical complexity measure
• DFA - Signal fractal scaling exponent
• PPE - A nonlinear measure of fundamental frequency variation

Divide it into training and test data (60/40) and scale it appropriately. In the
coming steps, assume that motor_UPDRS is normally distributed and is a
function of the voice characteristics, and since the data are scaled, no
intercept is needed in the modelling.
```{r, echo=F, fig.show='hide'}
library(caret)
parkin = read.csv("parkinsons.csv", header = T)
parkin_corr <- data.frame(parkin[c(5,7:22)]) #Remove unused voice characteristics
parkin_scaled <- as.data.frame(scale(parkin_corr))

n = dim(parkin_corr)[1]
set.seed(12345)

id=sample(1:n, floor(n*0.6))

train_pre=parkin_corr[id,]
test_pre=parkin_corr[-id,]

data_scaler<-preProcess(train_pre)
train<-predict(data_scaler,train_pre)
test<-predict(data_scaler,test_pre)
```


#Excercise 2

Compute a linear regression model from the training data, estimate training
and test MSE and comment on which variables contribute significantly to the
model.

```{r, echo=T, fig.show='hide'}
fit = lm(motor_UPDRS ~ .-1, data = train)
sum = summary(fit)
MSE_train<-mean(sum$residuals^2)
pred_test<-predict(fit,test)
MSE_test<-mean((test$motor_UPDRS-pred_test)^2)
print(MSE_train)
print(MSE_test)
print(sum)


```

The variables p-values that are higher than 0.05 does not contribute at all (insignificant P-values).
The values that contribute significantly are the ones with low p-values.
The variables that will contribute significantly to the model are the ones with the highest amount of stars in the table,
which represents the lowest p-values. 

MSE_train = 0.8785431
MSE_test = 0.9354477

#Exercise3

Implement 4 following functions by using basic R commands only (no
external packages):
a. Loglikelehood function that for a given parameter vector 𝜽𝜽 and
dispersion 𝜎𝜎 computes the log-likelihood function log 𝑃𝑃(𝑇𝑇|𝜽𝜽, 𝜎𝜎) for
the stated model and the training data

b. Ridge function that for given vector 𝜽𝜽, scalar 𝜎𝜎 and scalar 𝜆𝜆 uses
function from 3a and adds up a Ridge penalty 𝜆𝜆‖𝜽𝜽‖2 to the minus loglikelihood.

c. RidgeOpt function that depends on scalar 𝜆𝜆 , uses function from 3b
and function optim() with method=”BFGS” to find the optimal 𝜽𝜽 and 𝜎𝜎
for the given 𝜆𝜆.

d. Df function that for a given scalar 𝜆𝜆 computes the degrees of freedom
of the Ridge model based on the training data.

```{r, echo=F, fig.show='hide'}
log_likelihood <- function(train, Y, theta, sigma){

  n <- dim(train)[1] 
  train_theta = train%*%theta

  sum1 <- n*log(sigma^2)/2
  sum2 <- n*log(2*pi)/2
  sum3 <- sum((train_theta-Y)^2) 
  sum4 <- sum3/(2*sigma^2)
  
  return (-sum1-sum2-sum4)
}


ridge <- function(train, theta, lambda, Y){
  n<-dim(train)[2]
  sigma <- theta[n+1]
  theta <-as.matrix(theta[1:n])
  log_like <- log_likelihood(theta=theta,Y=Y,sigma=sigma,train=train)
  ridge <- -log_like + lambda*sum(theta^2)
  return(ridge)
  
}                                     

ridgeOpt <- function(lambda, train, Y){
  
  train <- as.matrix(train)
  N = dim(train)[2] 
  init_theta = integer(N)
  init_sigma = 1
  
  opt <- optim(par = c(init_theta,init_sigma), fn = ridge, lambda = lambda, train = train, Y = Y, method = "BFGS")
  return(opt)
} 

dF <- function(X, lambda){
  #From the course formula
  X <- as.matrix(X)
  Xt <- t(X)
  n <- dim(X)[2]
  I <- diag(n)
  P <- X%*%solve((Xt%*%X + (lambda*I)))%*%Xt
  return(sum(diag(P)))
} 
```

```{r, echo=F, fig.show='hide'}
  AIC = function(train,Y,theta, sigma, lambda){
    log_like = log_likelihood(train = train, Y=Y, theta = theta, sigma = sigma)
    N = dim(train)[1] # No of data points
    df = dF(train,lambda)
    aic = (-2*log_like/N) + (2*df/N) #(-2*Log-likelihood/N) + 2*(df/N)
    return(aic)
  }
```

#Exercise 4

By using function RidgeOpt, compute optimal 𝜽𝜽 parameters for 𝜆𝜆 = 1, 𝜆𝜆 =
100 and 𝜆𝜆 = 1000. Use the estimated parameters to predict the
motor_UPDRS values for training and test data and report the training and
test MSE values. Which penalty parameter is most appropriate among the
selected ones? Compute and compare the degrees of freedom of these models
and make appropriate conclusions.

```{r, echo=F, fig.show='hide'}
xtrain<-as.matrix(train[2:17])
ytrain<-as.matrix(train[1])
xtest=as.matrix(test[2:17])
ytest<-as.matrix(test[1])


for (lambda in c(1, 100, 1000)){
  opt = ridgeOpt(lambda, xtrain, ytrain)
  theta <- as.matrix(opt$par[1:16]) 
  sigma<- opt$par[17]
  MSE_train = mean((xtrain%*%theta - ytrain)^2)
  MSE_test = mean((xtest%*%theta - ytest)^2)
  aic = AIC(train=xtrain,Y= ytrain,theta = theta,sigma= sigma, lambda= lambda)
  print(paste("Lambda:",lambda))
  print(paste("TrainMSE:",MSE_train))
  print(paste("TestMSE:",MSE_test))
  print(paste("AIC:", aic)) #Unecessary


}

```

Which penalty parameter is most appropriate among the selected ones? Compute and compare the degrees of freedom of these models and make appropriate conclusions.

We have read that to find the best aic value among mutiple aic values is the lowest. In our example it seems like lambda = 1 and lambda = 100 gives similar aic values which is hard to draw conclusions from. Lambda = 1 gives the lowest AIC value.
Since the the Lambda = 100 has the lowest test_MSE, this is the best value. 
The degrees of freedom reflects the complexity of the model, so the larger lambda, the smaller degrees of freedom we would get. We get more complex models, basically penalizing the complexity. 


##Assignment 3.

The data file pima-indians-diabetes.csv contains information about the onset of
diabetes within 5 years in Pima Indians given medical details. The variables are (in
the same order as in the dataset):

Make a scatterplot showing a Plasma glucose concentration on Age where
observations are colored by Diabetes levels. Do you think that Diabetes is easy
to classify by a standard logistic regression model that uses these two variables as
features? Motivate your answer.

```{r, echo=F, fig.show='hide'}
prime = read.csv("pima-indians-diabetes.csv", header = F)

set.seed(12345)

```


```{r, echo=F, fig.show='hide'}
coloor <- function(x){
  if (x==1){
    c = "red"
  } else{
    c="green"
  } 
  return(c)
}  

coloors = sapply(prime$V9, coloor)
plot( prime$V2, prime$V8, xlab = "Plasma", ylab = "Age", main = "doabets", col = coloors)
```

seems to be different, no obvious correlation between the variables.
Making a linear regression would not provide a meaningful line. 


#Exercise 2

Train a logistic regression model with 𝑦𝑦 =Diabetes as target 𝑥𝑥1 =Plasma glucose
concentration and 𝑥𝑥2 =Age as features and make a prediction for all observations
by using 𝑟𝑟 = 0.5 as the classification threshold. Report the probabilistic equation
of the estimated model (i.e., how the target depends on the features and the
estimated model parameters probabilistically). Compute also the training
misclassification error and make a scatter plot of the same kind as in step 1 but
showing the predicted values of Diabetes as a color instead. Comment on the
quality of the classification by using these results

```{r, echo=F, fig.show='hide'}
glm.fits = glm(V9~ V2 + V8, prime, family = "binomial" )
prob=predict(glm.fits, type="response")
pred=ifelse(prob>0.5, 'red','green')

table = table(pred, prime$V9)

miss <- missclass(pred, prime$V9)

plot(prime$V2, prime$V8,col=pred, xlab = "Plasma glucose levels", ylab = "Age", main = paste("Missclass error", toString(miss), sep=" = ") ) 

table
#summary(table)
#summary(glm.fits)
glm.fits$coefficients



```

Probabilistic equation:
$$
p(y = 1|x_*) = g(x_*,\hat{\theta})= \frac{1}{1 + e^{-\hat{\theta}^Tx_*}} = \frac{1}{1 + e^{-(-5.91244906 + 0.03564404x_1 + 0.02477835x_2)}}
$$
It seems to be an acceptable classification with about 1/4 error but not perfect 
There seems to be a division when you reach above 150 in plasma-glucose levels. But this threshold seems to lower as you age. An older person is more likely to have diabetes even with lower plasma-glucose levels. 

#Exercise 3

Use the model estimated in step 2 to a) report the equation of the decision
boundary between the two classes b) add a curve showing this boundary to the
scatter plot in step 2. Comment whether the decision boundary seems to catch the
data distribution well.

```{r, echo=F, fig.show='hide'}

r=.5
glm.fits = glm(V9~ V2 + V8, prime, family = "binomial" )
cf = glm.fits$coefficients
prob=predict(glm.fits, type="response")
pred=ifelse(prob>0.5, 'red','green')

plot(prime$V2, prime$V8,col=pred, ylab = "Age", xlab= "Plasma", main = paste("Missclass Error", toString(miss), sep=" = ")) 

slope = coef(glm.fits)[2]/(-coef(glm.fits)[3])
intercept = (coef(glm.fits)[1] + log(-r/(r-1)))/(-coef(glm.fits)[3])

#clip(min(x1),max(x1), min(x2), max(x2))
abline(intercept , slope, lwd = 2, lty = 2)

he = log(-.2/(.2-1))
he
slope
intercept
```
y = -1.438516X + 238.6135

We can see how the distribution is hard to define and capture in the plot from the first assignment.
This attempt is a pretty good compromise, where most none diabetes are captured in the green to the left,
and most diabetes cases are captured to the right. But with a missclass error of 26% it still misses alot, which we can
also see when compared to the plot from the first assignment.


#Exercise 4

Make same kind of plots as in step 2 but use thresholds 𝑟𝑟 = 0.2 and 𝑟𝑟 = 0.8. By
using these plots, comment on what happens with the prediction when 𝑟𝑟 value
changes.

```{r, echo=F, fig.show='hide'}

for(r in c(.2,.5, .8)) {
  pred=ifelse(prob>r, 'red','green')
  table(pred, prime$V9)
  
  
  miss <- missclass(pred, prime$V9)
  
  plot(prime$V2, prime$V8,col=pred, ylab = "Age", xlab= "Plasma", main = paste("Missclass_Error = ", toString(miss), "\n r = ", r, sep= "")) 
  
  slope = coef(glm.fits)[2]/(-coef(glm.fits)[3])
  intercept = (coef(glm.fits)[1] - log(-r/(r-1)))/(-coef(glm.fits)[3])

  #clip(min(x1),max(x1), min(x2), max(x2))
  abline(intercept , slope, lwd = 2, lty = 2)
}
```

With r=.2 and r=.8, the missclassification error increased. r=.2 was clearly worse (37% error). 



#Exercise 5

Perform a basis function expansion trick by computing new features 𝑧𝑧1 = 𝑥𝑥1
4,
𝑧𝑧2 = 𝑥𝑥1
3𝑥𝑥2, 𝑧𝑧3 = 𝑥𝑥1
2𝑥𝑥2
2, 𝑧𝑧4 = 𝑥𝑥1𝑥𝑥2
3, 𝑧𝑧5 = 𝑥𝑥2
4 , adding them to the data set and
then computing a logistic regression model with 𝑦𝑦 as target and
𝑥𝑥1, 𝑥𝑥2, 𝑧𝑧1, … , 𝑧𝑧5 as features. Create a scatterplot of the same kind as in step 2
for this model and compute the training misclassification rate. What can you
say about the quality of this model compared to the previous logistic
regression model? How have the basis expansion trick affected the shape of
the decision boundary and the prediction accuracy?

```{r, echo=F, fig.show='hide'}
expanded <- prime

expanded$z1 <- expanded$V2 ** 4
expanded$z2 <- expanded$V2 ** 3 * expanded$V8
expanded$z3 <- expanded$V2 ** 2 * expanded$V8 ** 2
expanded$z4 <- expanded$V2 * expanded$V8 ** 3
expanded$z5 <- expanded$V8 ** 4

glm.fits = glm(V9~ V2 + V8 + z1 + z2 + z3 + z4 + z5, expanded, family = "binomial" )


for(r in c(.2,.5, .8)) {
  prob=predict(glm.fits, type="response")
  pred=ifelse(prob>r, 'red','green')
  table(pred, expanded$V9)
  
  miss <- missclass(pred, prime$V9)
  
  plot(expanded$V2, expanded$V8,col=pred, ylab = "Age", xlab= "Plasma", main = paste("Missclass_Error = ", toString(miss), "\n r = ", r, sep= "")) 
}
```


This model has a lower missclass error, so the quality of this model is slightly better than the previous one. 
The linearity of the model is however gone, since we've added non-linear variables. This causes the plotted colors to look more like a "slice"



