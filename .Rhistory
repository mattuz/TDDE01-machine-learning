for (j in 0:9){
cross_val <- valid$V65 == j
cross_train <- train$V65 == j
TRUE_val <- (which(cross_val, useNames = T))
prob_val <- k_valid$prob[cross_val, as.character(j)] + 1e-15
TRUE_train <- (which(cross_train, useNames = T))
prob_train <- k_train$prob[cross_train, as.character(j)] + 1e-15
val_miss_error[i] <- cross.entropy(TRUE_val, prob_val)
train_miss_error[i] <- cross.entropy(TRUE_train, prob_train)
entropy_error[i] <- abs(val_miss_error[i]-train_miss_error[i])
}
}
print(entropy_error)
plot(c(1:30), entropy_error, ylab = "cross_entropy_error", xlab = "k", col="blue")
which.min(entropy_error)
parkin = read.csv("parkinsons.csv", header = T)
parkin_corr <- data.frame(parkin[c(5,7:22)]) #Remove unused voice characteristics
parkin_scaled <- as.data.frame(scale(parkin_corr))
n = dim(parkin_corr)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.6))
train=parkin_scaled[id,]
test=parkin_scaled[-id,]
#Excercise 2
fit = lm(motor_UPDRS ~ ., data = train)
sum = summary(fit)
mean(sum$residuals^2)
print(sum)
# The variables p-values that are higher than 0.05 does not contribute at all.
#Exercise 3
#Loglikelihood
log_likelihood <- function(theta,Y, sigma,train){
mu = theta[0]
sigma2 = theta[1]
train <- as.matrix(train)
n <- dim(train)[1]
theta<-as.matrix(theta)
mu <- as.matrix(mu)
Y <- as.matrix(Y)
sum4 <- sum((train%*%Y-mu)^2)
sum1 <- n*log(sigma2)/2
sum2 <- n*log(2*pi)/2
sum3 <- sum4/(2*sigma2)
return (-sum1-sum2-sum3)
}
ridge <- function(train, theta, lambda, Y, sigma){
log_like <- log_likelihood(theta=theta,Y=Y,sigma=sigma,train=train)
ridge <- -log_lik + lambda*sum(theta[0]^2)
return(ridge)
}
ridgeOpt <- function(lambda, train, motomoto){
opt <- optim(par = c(0, 0), fn = ridge, lambda = lambda, train = train, Y = motomoto, method = "BFGS")
return(opt)
}
DF <- function(lambda, X){
#From the course formula
X <- as.matrix(X)
Xt <- t(X)
n <- dim(X)[2]
I <- diag(n)
P <- X%*%solve((Xt%*%X + lambda%*%I))%*%Xt
return(t(P))
}
#Exercise 4
Xtrain<-as.matrix(train[2:17])
motomoto<-as.matrix(train[1])
Xtest=as.matrix(test[2:17])
Ytest<-as.matrix(test[1])
for (lambda in c(1, 100, 1000)){
ridge_opt = ridgeOpt(lambda,Xtrain, motomoto)
print(ridge_opt)
}
parkin = read.csv("parkinsons.csv", header = T)
parkin_corr <- data.frame(parkin[c(5,7:22)]) #Remove unused voice characteristics
parkin_scaled <- as.data.frame(scale(parkin_corr))
n = dim(parkin_corr)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.6))
train=parkin_scaled[id,]
test=parkin_scaled[-id,]
#Excercise 2
fit = lm(motor_UPDRS ~ ., data = train)
sum = summary(fit)
mean(sum$residuals^2)
print(sum)
# The variables p-values that are higher than 0.05 does not contribute at all.
#Exercise 3
#Loglikelihood
log_likelihood <- function(theta,Y, sigma,train){
print(train)
print(Y)
mu = theta[0]
sigma2 = theta[1]
train <- as.matrix(train)
n <- dim(train)[1]
theta<-as.matrix(theta)
mu <- as.matrix(mu)
Y <- as.matrix(Y)
sum4 <- sum((train%*%Y-mu)^2)
sum1 <- n*log(sigma2)/2
sum2 <- n*log(2*pi)/2
sum3 <- sum4/(2*sigma2)
return (-sum1-sum2-sum3)
}
ridge <- function(train, theta, lambda, Y, sigma){
log_like <- log_likelihood(theta=theta,Y=Y,sigma=sigma,train=train)
ridge <- -log_lik + lambda*sum(theta[0]^2)
return(ridge)
}
ridgeOpt <- function(lambda, train, motomoto){
opt <- optim(par = c(0, 0), fn = ridge, lambda = lambda, train = train, Y = motomoto, method = "BFGS")
return(opt)
}
DF <- function(lambda, X){
#From the course formula
X <- as.matrix(X)
Xt <- t(X)
n <- dim(X)[2]
I <- diag(n)
P <- X%*%solve((Xt%*%X + lambda%*%I))%*%Xt
return(t(P))
}
#Exercise 4
Xtrain<-as.matrix(train[2:17])
motomoto<-as.matrix(train[1])
Xtest=as.matrix(test[2:17])
Ytest<-as.matrix(test[1])
for (lambda in c(1, 100, 1000)){
ridge_opt = ridgeOpt(lambda,Xtrain, motomoto)
print(ridge_opt)
}
parkin = read.csv("parkinsons.csv", header = T)
parkin_corr <- data.frame(parkin[c(5,7:22)]) #Remove unused voice characteristics
parkin_scaled <- as.data.frame(scale(parkin_corr))
n = dim(parkin_corr)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.6))
train=parkin_scaled[id,]
test=parkin_scaled[-id,]
#Excercise 2
fit = lm(motor_UPDRS ~ ., data = train)
sum = summary(fit)
mean(sum$residuals^2)
print(sum)
# The variables p-values that are higher than 0.05 does not contribute at all.
#Exercise 3
#Loglikelihood
log_likelihood <- function(theta,Y, sigma,train){
print(Y)
mu = theta[0]
sigma2 = theta[1]
train <- as.matrix(train)
n <- dim(train)[1]
theta<-as.matrix(theta)
mu <- as.matrix(mu)
Y <- as.matrix(Y)
sum4 <- sum((train%*%Y-mu)^2)
sum1 <- n*log(sigma2)/2
sum2 <- n*log(2*pi)/2
sum3 <- sum4/(2*sigma2)
return (-sum1-sum2-sum3)
}
ridge <- function(train, theta, lambda, Y, sigma){
log_like <- log_likelihood(theta=theta,Y=Y,sigma=sigma,train=train)
ridge <- -log_lik + lambda*sum(theta[0]^2)
return(ridge)
}
ridgeOpt <- function(lambda, train, motomoto){
opt <- optim(par = c(0, 0), fn = ridge, lambda = lambda, train = train, Y = motomoto, method = "BFGS")
return(opt)
}
DF <- function(lambda, X){
#From the course formula
X <- as.matrix(X)
Xt <- t(X)
n <- dim(X)[2]
I <- diag(n)
P <- X%*%solve((Xt%*%X + lambda%*%I))%*%Xt
return(t(P))
}
#Exercise 4
Xtrain<-as.matrix(train[2:17])
motomoto<-as.matrix(train[1])
Xtest=as.matrix(test[2:17])
Ytest<-as.matrix(test[1])
for (lambda in c(1, 100, 1000)){
ridge_opt = ridgeOpt(lambda,Xtrain, motomoto)
print(ridge_opt)
}
parkin = read.csv("parkinsons.csv", header = T)
parkin_corr <- data.frame(parkin[c(5,7:22)]) #Remove unused voice characteristics
parkin_scaled <- as.data.frame(scale(parkin_corr))
n = dim(parkin_corr)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.6))
train=parkin_scaled[id,]
test=parkin_scaled[-id,]
#Excercise 2
fit = lm(motor_UPDRS ~ ., data = train)
sum = summary(fit)
mean(sum$residuals^2)
print(sum)
# The variables p-values that are higher than 0.05 does not contribute at all.
#Exercise 3
#Loglikelihood
log_likelihood <- function(theta,Y, sigma,train){
print(train)
mu = theta[0]
sigma2 = theta[1]
train <- as.matrix(train)
n <- dim(train)[1]
theta<-as.matrix(theta)
mu <- as.matrix(mu)
Y <- as.matrix(Y)
sum4 <- sum((train%*%Y-mu)^2)
sum1 <- n*log(sigma2)/2
sum2 <- n*log(2*pi)/2
sum3 <- sum4/(2*sigma2)
return (-sum1-sum2-sum3)
}
ridge <- function(train, theta, lambda, Y, sigma){
log_like <- log_likelihood(theta=theta,Y=Y,sigma=sigma,train=train)
ridge <- -log_lik + lambda*sum(theta[0]^2)
return(ridge)
}
ridgeOpt <- function(lambda, train, motomoto){
opt <- optim(par = c(0, 0), fn = ridge, lambda = lambda, train = train, Y = motomoto, method = "BFGS")
return(opt)
}
DF <- function(lambda, X){
#From the course formula
X <- as.matrix(X)
Xt <- t(X)
n <- dim(X)[2]
I <- diag(n)
P <- X%*%solve((Xt%*%X + lambda%*%I))%*%Xt
return(t(P))
}
#Exercise 4
Xtrain<-as.matrix(train[2:17])
motomoto<-as.matrix(train[1])
Xtest=as.matrix(test[2:17])
Ytest<-as.matrix(test[1])
for (lambda in c(1, 100, 1000)){
ridge_opt = ridgeOpt(lambda,Xtrain, motomoto)
print(ridge_opt)
}
parkin = read.csv("parkinsons.csv", header = T)
parkin_corr <- data.frame(parkin[c(5,7:22)]) #Remove unused voice characteristics
parkin_scaled <- as.data.frame(scale(parkin_corr))
n = dim(parkin_corr)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.6))
train=parkin_scaled[id,]
test=parkin_scaled[-id,]
#Excercise 2
fit = lm(motor_UPDRS ~ ., data = train)
sum = summary(fit)
mean(sum$residuals^2)
print(sum)
# The variables p-values that are higher than 0.05 does not contribute at all.
#Exercise 3
#Loglikelihood
log_likelihood <- function(theta,Y, sigma,train){
print(Y)
mu = theta[0]
sigma2 = theta[1]
train <- as.matrix(train)
n <- dim(train)[1]
theta<-as.matrix(theta)
mu <- as.matrix(mu)
Y <- as.matrix(Y)
sum4 <- sum((train%*%Y-mu)^2)
sum1 <- n*log(sigma2)/2
sum2 <- n*log(2*pi)/2
sum3 <- sum4/(2*sigma2)
return (-sum1-sum2-sum3)
}
ridge <- function(train, theta, lambda, Y, sigma){
log_like <- log_likelihood(theta=theta,Y=Y,sigma=sigma,train=train)
ridge <- -log_lik + lambda*sum(theta[0]^2)
return(ridge)
}
ridgeOpt <- function(lambda, train, motomoto){
opt <- optim(par = c(0, 0), fn = ridge, lambda = lambda, train = train, Y = motomoto, method = "BFGS")
return(opt)
}
DF <- function(lambda, X){
#From the course formula
X <- as.matrix(X)
Xt <- t(X)
n <- dim(X)[2]
I <- diag(n)
P <- X%*%solve((Xt%*%X + lambda%*%I))%*%Xt
return(t(P))
}
#Exercise 4
Xtrain<-as.matrix(train[2:17])
motomoto<-as.matrix(train[1])
Xtest=as.matrix(test[2:17])
Ytest<-as.matrix(test[1])
for (lambda in c(1, 100, 1000)){
ridge_opt = ridgeOpt(lambda,Xtrain, motomoto)
print(ridge_opt)
}
dim(Y)[2]
log_likelihood <- function(theta,Y, sigma,train){
print(Y)
mu = theta[0]
sigma2 = theta[1]
train <- as.matrix(train)
n <- dim(train)[1]
theta<-as.matrix(theta)
mu <- as.matrix(mu)
Y <- as.matrix(Y)
dim(Y)[2]
dim(X)[1]
#Loglikelihood
log_likelihood <- function(theta,Y, sigma,train){
print(Y)
mu = theta[0]
sigma2 = theta[1]
train <- as.matrix(train)
n <- dim(train)[1]
theta<-as.matrix(theta)
mu <- as.matrix(mu)
Y <- as.matrix(Y)
print(dim(Y)[2] )
print(dim(X)[1])
parkin = read.csv("parkinsons.csv", header = T)
parkin_corr <- data.frame(parkin[c(5,7:22)]) #Remove unused voice characteristics
parkin_scaled <- as.data.frame(scale(parkin_corr))
n = dim(parkin_corr)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.6))
train=parkin_scaled[id,]
test=parkin_scaled[-id,]
#Excercise 2
fit = lm(motor_UPDRS ~ ., data = train)
sum = summary(fit)
mean(sum$residuals^2)
print(sum)
# The variables p-values that are higher than 0.05 does not contribute at all.
#Exercise 3
#Loglikelihood
log_likelihood <- function(theta,Y, sigma,train){
print(Y)
mu = theta[0]
sigma2 = theta[1]
train <- as.matrix(train)
n <- dim(train)[1]
theta<-as.matrix(theta)
mu <- as.matrix(mu)
Y <- as.matrix(Y)
print(dim(Y)[2] )
print(dim(X)[1])
sum4 <- sum((train%*%-mu)^2)
sum1 <- n*log(sigma2)/2
sum2 <- n*log(2*pi)/2
sum3 <- sum4/(2*sigma2)
return (-sum1-sum2-sum3)
}
ridge <- function(train, theta, lambda, Y, sigma){
log_like <- log_likelihood(theta=theta,Y=Y,sigma=sigma,train=train)
ridge <- -log_lik + lambda*sum(theta[0]^2)
return(ridge)
}
ridgeOpt <- function(lambda, train, motomoto){
opt <- optim(par = c(0, 0), fn = ridge, lambda = lambda, train = train, Y = motomoto, method = "BFGS")
return(opt)
}
DF <- function(lambda, X){
#From the course formula
X <- as.matrix(X)
Xt <- t(X)
n <- dim(X)[2]
I <- diag(n)
P <- X%*%solve((Xt%*%X + lambda%*%I))%*%Xt
return(t(P))
}
#Exercise 4
Xtrain<-as.matrix(train[2:17])
motomoto<-as.matrix(train[1])
Xtest=as.matrix(test[2:17])
Ytest<-as.matrix(test[1])
for (lambda in c(1, 100, 1000)){
ridge_opt = ridgeOpt(lambda,Xtrain, motomoto)
print(ridge_opt)
}
for (lambda in c(1, 100, 1000)){
ridge_opt = ridgeOpt(lambda,Xtrain, motomoto)
print(ridge_opt)
dev.off()
parkin = read.csv("parkinsons.csv", header = T)
parkin_corr <- data.frame(parkin[c(5,7:22)]) #Remove unused voice characteristics
parkin_scaled <- as.data.frame(scale(parkin_corr))
n = dim(parkin_corr)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.6))
train=parkin_scaled[id,]
test=parkin_scaled[-id,]
#Excercise 2
fit = lm(motor_UPDRS ~ ., data = train)
sum = summary(fit)
mean(sum$residuals^2)
print(sum)
# The variables p-values that are higher than 0.05 does not contribute at all.
#Exercise 3
#Loglikelihood
log_likelihood <- function(theta,Y, sigma,train){
print(Y)
mu = theta[0]
sigma2 = theta[1]
train <- as.matrix(train)
n <- dim(train)[1]
theta<-as.matrix(theta)
mu <- as.matrix(mu)
Y <- as.matrix(Y)
print(dim(Y)[2] )
print(dim(X)[1])
sum4 <- sum((train%*%-mu)^2)
sum1 <- n*log(sigma2)/2
sum2 <- n*log(2*pi)/2
sum3 <- sum4/(2*sigma2)
return (-sum1-sum2-sum3)
}
ridge <- function(train, theta, lambda, Y, sigma){
log_like <- log_likelihood(theta=theta,Y=Y,sigma=sigma,train=train)
ridge <- -log_lik + lambda*sum(theta[0]^2)
return(ridge)
}
ridgeOpt <- function(lambda, train, motomoto){
opt <- optim(par = c(0, 0), fn = ridge, lambda = lambda, train = train, Y = motomoto, method = "BFGS")
return(opt)
}
DF <- function(lambda, X){
#From the course formula
X <- as.matrix(X)
Xt <- t(X)
n <- dim(X)[2]
I <- diag(n)
P <- X%*%solve((Xt%*%X + lambda%*%I))%*%Xt
return(t(P))
}
#Exercise 4
Xtrain<-as.matrix(train[2:17])
motomoto<-as.matrix(train[1])
Xtest=as.matrix(test[2:17])
Ytest<-as.matrix(test[1])
for (lambda in c(1, 100, 1000)){
ridge_opt = ridgeOpt(lambda,Xtrain, motomoto)
print(ridge_opt)
}
parkin = read.csv("parkinsons.csv", header = T)
parkin_corr <- data.frame(parkin[c(5,7:22)]) #Remove unused voice characteristics
parkin_scaled <- as.data.frame(scale(parkin_corr))
n = dim(parkin_corr)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.6))
train=parkin_scaled[id,]
test=parkin_scaled[-id,]
#Excercise 2
fit = lm(motor_UPDRS ~ ., data = train)
sum = summary(fit)
mean(sum$residuals^2)
print(sum)
# The variables p-values that are higher than 0.05 does not contribute at all.
#Exercise 3
#Loglikelihood
log_likelihood <- function(theta,Y, sigma,train){
mu = theta[0]
sigma2 = theta[1]
train <- as.matrix(train)
n <- dim(train)[1]
theta<-as.matrix(theta)
mu <- as.matrix(mu)
Y <- as.matrix(Y)
print(dim(Y)[2] )
print(dim(X)[1])
sum4 <- sum((train%*%-mu)^2)
sum1 <- n*log(sigma2)/2
sum2 <- n*log(2*pi)/2
sum3 <- sum4/(2*sigma2)
return (-sum1-sum2-sum3)
}
ridge <- function(train, theta, lambda, Y, sigma){
log_like <- log_likelihood(theta=theta,Y=Y,sigma=sigma,train=train)
ridge <- -log_lik + lambda*sum(theta[0]^2)
return(ridge)
}
ridgeOpt <- function(lambda, train, motomoto){
opt <- optim(par = c(0, 0), fn = ridge, lambda = lambda, train = train, Y = motomoto, method = "BFGS")
return(opt)
}
DF <- function(lambda, X){
#From the course formula
X <- as.matrix(X)
Xt <- t(X)
n <- dim(X)[2]
I <- diag(n)
P <- X%*%solve((Xt%*%X + lambda%*%I))%*%Xt
return(t(P))
}
#Exercise 4
Xtrain<-as.matrix(train[2:17])
motomoto<-as.matrix(train[1])
Xtest=as.matrix(test[2:17])
Ytest<-as.matrix(test[1])
for (lambda in c(1, 100, 1000)){
ridge_opt = ridgeOpt(lambda,Xtrain, motomoto)
print(ridge_opt)
}
